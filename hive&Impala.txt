-- System--
describe function extended to_date;
set system:sun.java.command;
set -v;
SELECT FROM_UNIXTIME(UNIX_TIMESTAMP());

---------------------------------------------------------
-- TODO: Need to sort out the partition
set mapred.output.compress=true;
set mapred.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;
set hive.exec.compress.output=true;
set avro.output.codec=snappy;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.error.on.empty.partition=false;

-- Uncompressed TSV files will be retrieved from S3 and loaded to HDFS
-- The new file(s) will be loaded to a load_date based partition of tv_api_raw
-- Files will contain a header record that will need to be catered for in the table definition
-- A file containing data from 11 Nov 2015 through to the current date will be provided upon request
-- -- to populate the table with "historical" data
-- Original files will be retained in a separate S3 bucket. Bucket policies may be applied by SWM
-- -- to delete this data after 30 days (source system remains available to regenerate data if necessary).

use ${database_to_use};

CREATE EXTERNAL TABLE IF NOT EXISTS ${table_prefix}_raw (

    record_type                   STRING
    , broadcast_date              STRING
    , market	                    STRING
    , channel	                    STRING
    , day_part	                  STRING
    , target	                    STRING
    , variable	                  STRING
    , overnight	                  STRING
    , timeshift	                  STRING
    , consolidated	              STRING
)

PARTITIONED BY (load_date STRING)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
STORED AS TEXTFILE
LOCATION '${hdfs_table_location}'
tblproperties ("skip.header.line.count"="4");

hdfs_table_location=${data_area}/raw/${file_area}_raw
database_to_use=${database_to_use}



set mapred.output.compress=true;
set mapred.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;
set hive.exec.compress.output=true;
set avro.output.codec=snappy;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.error.on.empty.partition=false;

use ${database_to_use};
ALTER TABLE ${table_prefix}_raw ADD IF NOT EXISTS PARTITION (load_date='${load_date}');
LOAD DATA INPATH '${tmp_table}/*' INTO TABLE ${table_prefix}_raw PARTITION (load_date='${load_date}');

load_date=${load_date}
database_to_use=${database_to_use}
tmp_table=${data_area}/tmp/${file_area}_raw
table_prefix=${file_area}

alter table redfusion_explore.dfp_impressions_raw
ADD IF NOT EXISTS PARTITION (load_date='20180508')
location '/user/tgao/raw/load_date=20180508';

alter table redfusion_explore.dfp_impressions_raw drop partition (load_date='20180508');
show partitions redfusion_explore.dfp_impressions_raw;

alter table redfusion_explore.dfp_impressions_raw set tblproperties('EXTERNAL'='FALSE');
------------------------------------------------------------------------------------------------------------


CREATE EXTERNAL TABLE IF NOT EXISTS ${table_prefix}_curated (
    broadcast_date                STRING
    , market	                    STRING
    , channel	                    STRING
    , day_part	                  STRING
    , target	                    STRING
    , variable	                  STRING
    , overnight	                  INT
    , timeshift	                  INT
    , consolidated	              INT
)

PARTITIONED BY (load_date STRING)
STORED AS PARQUET
LOCATION '${hdfs_table_location}'
;

hdfs_table_location=${data_area}/curated/${file_area}_curated


set mapred.output.compress=true;
set mapred.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;
set hive.exec.compress.output=true;
set avro.output.codec=snappy;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.error.on.empty.partition=false;


USE ${database_to_use};

INSERT INTO TABLE ${table_prefix}_curated
PARTITION (load_date='${load_date}')
SELECT

t1.broadcast_date,
t1.market,
t1.channel,
t1.day_part,
t1.target,
t1.variable,
cast(regexp_replace(t1.overnight, ',', '') as INT) overnight,
cast(regexp_replace(t1.timeshift, ',', '') as INT) timeshift,
cast(regexp_replace(t1.consolidated, ',', '') as INT) consolidated

FROM ${table_prefix}_raw t1
WHERE load_date='${load_date}'
AND record_type='Data'
;

------------------------------------------------------


CREATE EXTERNAL TABLE IF NOT EXISTS ${table_prefix}_model (
    broadcast_date                STRING
    , market	                    STRING
    , channel	                    STRING
    , day_part	                  STRING
    , target	                    STRING
    , event_tv_start_min_local    STRING
    , stream_channel	            STRING
    , total_overnight	            INT
    , total_timeshift	            INT
    , total_consolidated          INT
)

PARTITIONED BY (load_date STRING)
STORED AS PARQUET
LOCATION '${hdfs_table_location}'
;

---------------------------------------------------------------------

#!/bin/bash -e

IMPALA_D=$1
DATABASE_TO_USE=$2
FILE_TO_RUN=$3
TABLE_PREFIX=$4


MY_QUERY="`hdfs dfs -cat $FILE_TO_RUN`"
export PYTHON_EGG_CACHE=/home/.python-eggs

replacement_db_string=$2
replacement_table_prefix_string=$4

original_string="$MY_QUERY"
string_to_replace_db_with='${database_to_use}'
string_to_replace_table_prefix_with='${table_prefix}'

# NOTE: an asterix (or Obelix) in the query will substitute all bash arguements instead - BE CAREFUL IN YOUR SCRIPT.
result_string="${original_string//$string_to_replace_db_with/$replacement_db_string}"
result_string2="${result_string//$string_to_replace_table_prefix_with/$replacement_table_prefix_string}"


#clear
#echo "--1-- " $MY_QUERY
#echo "--2-- " $result_string
echo $result_string2

impala-shell -i $IMPALA_D:21000 --database=$DATABASE_TO_USE --query="$result_string2"



USE ${database_to_use};

INVALIDATE METADATA tv_min_events_curated;
INVALIDATE METADATA tmp_tv_min_events_xfm_view;

INSERT INTO TABLE tv_min_events_model
PARTITION(load_date)
  SELECT

  v.broadcast_date,
  v.market,
  v.channel,
  v.day_part,
  v.target,
  v.event_tv_start_min_local,
  v.stream_channel,
  v.total_overnight,
  v.total_timeshift,
  v.total_consolidated,
  v.load_date

FROM tmp_tv_min_events_xfm_view v
WHERE ingest_date='${load_date}';
INVALIDATE METADATA tv_min_events_model;

----------------------------------

Impala :

use ${database_to_use};

DROP VIEW IF EXISTS tmp_${table_prefix}_xfm_view;
CREATE VIEW IF NOT EXISTS tmp_${table_prefix}_xfm_view AS
SELECT

    concat(substr(broadcast_date,7,4),'-',substr(broadcast_date,4,2),'-',substr(broadcast_date,1,2)) broadcast_date,
    market,
    CASE
     WHEN lower(v.channel) LIKE '%channel 7%' THEN 'seven'
     WHEN lower(v.channel) LIKE '%7flix%' THEN '7flix'
     WHEN lower(v.channel) LIKE '%7two%' THEN '7two'
     WHEN lower(v.channel) LIKE '%7mate%' THEN '7mate'
     WHEN lower(v.channel) LIKE '%prime7/7qld/gwn7%' THEN 'seven'
       WHEN lower(v.channel) LIKE '%southern cross tas%' THEN 'seven'
       ELSE lower(v.channel)
    END as channel,
    day_part,
    target,
    CASE
     WHEN length(CAST(CAST(substr(day_part,1,2) AS INT) - 2 AS STRING)) = 2 THEN CAST(hours_add(CAST(concat(substr(broadcast_date,7,4),'-',substr(broadcast_date, 4, 2),'-', substr(broadcast_date,1,2),
     ' ',(concat(CAST(CAST(substr(day_part,1,2) AS INT) - 2 AS STRING), substr(day_part,3,6)))) AS TIMESTAMP),2) AS STRING)
     WHEN length(CAST(CAST(substr(day_part,1,2) AS INT) - 2 AS STRING)) = 1 THEN concat(substr(broadcast_date,7,4),
     '-',substr(broadcast_date,4,2),'-',substr(broadcast_date,1,2),
     ' ',substr(day_part,1,8))
    END event_tv_start_min_local,
    concat(CASE
     WHEN lower(v.channel) LIKE '%channel 7%' THEN '7'
     WHEN lower(v.channel) LIKE '%7flix%' THEN '7flix'
     WHEN lower(v.channel) LIKE '%7two%' THEN '7two'
     WHEN lower(v.channel) LIKE '%7mate%' THEN '7mate'
     WHEN lower(v.channel) LIKE '%prime7/7qld/gwn7%' THEN '7'
     WHEN lower(v.channel) LIKE '%southern cross tas%' THEN '7'
     ELSE lower(v.channel)
    END, ' ',CASE
     WHEN lower(market) in ('northern nsw','southern nsw')  THEN 'sydney'
     WHEN lower(market) = 'western australia'  THEN 'perth'
     WHEN lower(market) in ('victoria','tasmania') THEN 'melbourne'
     WHEN lower(market) = 'queensland' THEN 'brisbane'
     ELSE lower(market)
    END) stream_channel,
    overnight total_overnight,
    timeshift total_timeshift,
    consolidated total_consolidated,
    load_date as ingest_date,
    concat(substr(broadcast_date,7,4),substr(broadcast_date,4,2),substr(broadcast_date,1,2))  as load_date

  FROM
     ${table_prefix}_curated v
  ;


-------------------------------------


COMPUTE INCREMENTAL STATS amd_events_curated;
COMPUTE INCREMENTAL STATS amd_events_session_values;
-- COMPUTE INCREMENTAL STATS amd_events_model;

COMPUTE STATS content_media_model;
-- No workflow, not currently in use
-- COMPUTE INCREMENTAL STATS presto_curated;
-- COMPUTE INCREMENTAL STATS presto_events_model;

-- wf_amd_device
-- Use of incremental stats for amd_device_curated may have
-- contributed to issue whereby queries would fail (2016-07-07).
-- Reverting to full stats and restarting impala resolved
-- problem.
-- COMPUTE INCREMENTAL STATS amd_device_curated;
COMPUTE STATS amd_device_curated;
-- ==== Nothing below here is backed up ====
-- wf_tv_api
COMPUTE INCREMENTAL STATS tv_api_raw;
COMPUTE INCREMENTAL STATS live_media_model;
-- wf_events_int
COMPUTE INCREMENTAL STATS live_events_int;
COMPUTE INCREMENTAL STATS vod_events_int;
-- wf_vpm_min_agg
COMPUTE INCREMENTAL STATS vpm_min_agg;
-- wf_amd_agg
COMPUTE INCREMENTAL STATS live_vpm_hourly_agg;
COMPUTE INCREMENTAL STATS vod_vpm_hourly_agg;

-- wf_tv_min_events
COMPUTE INCREMENTAL STATS tv_min_events_curated;
COMPUTE INCREMENTAL STATS tv_min_events_model;
COMPUTE INCREMENTAL STATS tv_demo_events_curated;
COMPUTE INCREMENTAL STATS tv_demo_events_model;
-- wf_pre_rating
COMPUTE INCREMENTAL STATS pre_rating_log_curated;

-- Used in views within wf_events_int & wf_amd_agg
-- COMPUTE STATS obs_discipline_lookup;
-- COMPUTE STATS obs_event_lookup;
-- COMPUTE STATS obs_event_unit_lookup;
-- COMPUTE STATS rio_event_broadcast_log;
-- COMPUTE STATS rio_event_obs_log;
-- COMPUTE STATS stream_market;
-- COMPUTE STATS postcode_metro;

-- wf_cms_log
COMPUTE INCREMENTAL STATS cms_media_model;

-- wf_match_event_log
COMPUTE INCREMENTAL STATS ausopen_event_log_model;

-- wf_dfp_impressions
COMPUTE INCREMENTAL STATS dfp_events_curated;
COMPUTE INCREMENTAL STATS lineitem_lookup;

-- wf_lotame_dmp
COMPUTE INCREMENTAL STATS lot_audiences_curated;
COMPUTE INCREMENTAL STATS lot_behaviours_raw;
COMPUTE INCREMENTAL STATS lot_sites_curated;
COMPUTE INCREMENTAL STATS lot_sites_raw;

-- teradata import tables
COMPUTE INCREMENTAL STATS ti_lotame_auds_lookup;
COMPUTE INCREMENTAL STATS ti_lotame_auds_members;

-- misc tables as per Cloudera suggestion
COMPUTE INCREMENTAL STATS adunit_parent_lookup;

-- Impala via Oozie does not like to end on a comment so here
-- is a dummy query.
select now();
-------------------------------------------------------

Input is JSON File :

DROP TABLE amd_device_raw;

add jar /opt/local/hive/lib/json-serde-1.3.7-jar-with-dependencies.jar;

CREATE EXTERNAL TABLE amd_device_raw (
        updatedOn       STRING,
        deviceId        STRING,
        publisherId     STRING,
        publisherName   STRING,
        country         STRING,
        countryName     STRING,
        region          STRING,
        regionName      STRING,
        city            STRING,
        postalCode      STRING,
        location        STRUCT <
                lat :STRING,
                lon :STRING
        >,
        browser         STRING,
        platform        STRING,
        os              STRING,
        version         STRING,
        deviceType      STRING
)
PARTITIONED BY (load_date STRING)
ROW FORMAT SERDE "org.openx.data.jsonserde.JsonSerDe"
STORED AS TEXTFILE LOCATION "/appdata/redfusion/prod/raw/amd_device_raw";

MSCK REPAIR TABLE amd_device_raw;


ADD JAR ${json_serde_to_use};

set mapred.output.compress=true;
set mapred.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;
set hive.exec.compress.output=true;
set avro.output.codec=snappy;
set hive.error.on.empty.partition=false;

USE ${database_to_use};

CREATE EXTERNAL TABLE IF NOT EXISTS amd_device_curated
    PARTITIONED BY (load_date STRING)
    ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
    STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
    OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
    LOCATION '${hdfs_table_location}'
    TBLPROPERTIES ('avro.schema.url'='${json_schema_location}');

json_schema_location=${nameNode}${data_area}/json_schema_files/schema_amd_device_curated.json

{
    "type":"record",
    "name":"amd_device_curated",
    "namespace":"redfusion",
    "fields":[
        {"name":"updated_on","type":["null","string"],"default":null,"doc":"amd_device_raw.updatedOn"},
        {"name":"device_id","type":["null","string"],"default":null,"doc":"amd_device_raw.deviceId"},
        {"name":"publisher_id","type":["null","string"],"default":null,"doc":"amd_device_raw.publisherId"},
        {"name":"publisher_name","type":["null","string"],"default":null,"doc":"amd_device_raw.publisherName"},
        {"name":"country","type":["null","string"],"default":null,"doc":"amd_device_raw.country"},
        {"name":"country_name","type":["null","string"],"default":null,"doc":"amd_device_raw.countryName"},
        {"name":"region","type":["null","string"],"default":null,"doc":"amd_device_raw.region"},
        {"name":"region_name","type":["null","string"],"default":null,"doc":"amd_device_raw.regionName"},
        {"name":"city","type":["null","string"],"default":null,"doc":"amd_device_raw.city"},
        {"name":"post_code","type":["null","string"],"default":null,"doc":"amd_device_raw.postalCode"},
        {"name":"latitude","type":["null","double"],"default":null,"doc":"amd_device_raw.location.lat"},
        {"name":"longitude","type":["null","double"],"default":null,"doc":"amd_device_raw.location.lon"},
        {"name":"browser","type":["null","string"],"default":null,"doc":"amd_device_raw.browser"},
        {"name":"platform","type":["null","string"],"default":null,"doc":"amd_device_raw.platform"},
        {"name":"os","type":["null","string"],"default":null,"doc":"amd_device_raw.os"},
        {"name":"version","type":["null","string"],"default":null,"doc":"amd_device_raw.version"},
        {"name":"device_type","type":["null","string"],"default":null,"doc":"amd_device_raw.deviceType"}
    ]
}



ADD JAR ${json_serde_to_use};

set mapred.output.compress=true;
set mapred.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;
set hive.exec.compress.output=true;
set avro.output.codec=snappy;

set hive.exec.dynamic.partition.mode=nonstrict;

USE ${database_to_use};

CREATE EXTERNAL TABLE IF NOT EXISTS amd_raw (
  ts              STRING,
  timezoneOffset  STRING,
  sessionId       STRING,
  publisherId     STRING,
  mediaId         STRING,
  mediaDuration   STRING,
  mediaType       STRING,
  plugin          STRING,
  vendorVersion   STRING,
  protocolVersion STRING,
  ipAddress       STRING,
  url             STRING,
  id              STRING,
  createdAt       STRING,
  deviceId        STRING,  
  userAgent STRUCT <
    browser         :STRING,
    os              :STRING,
    platform        :STRING,
    version         :STRING
  >,
  properties STRUCT <
    altMediaId      :STRING,
    programId       :STRING,
    programName     :STRING,
    seriesId        :STRING,
    seriesName      :STRING,
    episodeId       :STRING,
    episodeName     :STRING,
    genre           :STRING,
    channel         :STRING,
    classification  :STRING,
    dvbTriplet      :STRING,
    latitude        :STRING,
    longitude       :STRING,
    postCode        :STRING,
    demo1           :STRING,
    demo2           :STRING,
    demo3           :STRING,
    connectionType  :STRING,
    streamingType   :STRING,
    deviceId        :STRING
  >,
  events ARRAY <
    STRUCT <
      event         :STRING,
      ts            :STRING,
      fromPosition  :STRING,
      toPosition    :STRING
    >
  >
)
PARTITIONED BY (load_date STRING)
ROW FORMAT SERDE "org.openx.data.jsonserde.JsonSerDe"
WITH SERDEPROPERTIES ("mapping.ts" = "timestamp")
STORED AS TEXTFILE LOCATION '${hdfs_table_location}';



ADD JAR ${json_serde_to_use};

set mapred.output.compress=true;
set mapred.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;
set hive.exec.compress.output=true;
set avro.output.codec=snappy;
set hive.error.on.empty.partition=false;

USE ${database_to_use};

CREATE EXTERNAL TABLE IF NOT EXISTS amd_events_curated
    PARTITIONED BY (load_date STRING)
    ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
    STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
    OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
    LOCATION '${hdfs_table_location}'
    TBLPROPERTIES ('avro.schema.url'='${json_schema_location}');


set avro.output.codec=snappy;
set hive.error.on.empty.partition=false;

USE ${database_to_use};

-- The following works for Hive *and* Impala.
CREATE EXTERNAL TABLE IF NOT EXISTS content_media_model
    ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
    STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
    OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
    LOCATION '${hdfs_table_location}'
    TBLPROPERTIES ('avro.schema.literal'='{
        "type":"record",
        "name":"content_media_model",
        "namespace":"redfusion_dev",
        "fields":[
            {"name":"media_id","type":["null","string"],"default":null},
            {"name":"publisher_id","type":["null","string"],"default":null},
            {"name":"media_duration","type":["null","double"],"default":null},
            {"name":"alt_media_id","type":["null","string"],"default":null},
            {"name":"media_type","type":["null","string"],"default":null},
            {"name":"program_id","type":["null","string"],"default":null},
            {"name":"program_name","type":["null","string"],"default":null},
            {"name":"series_id","type":["null","string"],"default":null},
            {"name":"series_name","type":["null","string"],"default":null},
            {"name":"episode_id","type":["null","string"],"default":null},
            {"name":"episode_name","type":["null","string"],"default":null},
            {"name":"genre","type":["null","string"],"default":null},
            {"name":"sub_genre","type":["null","string"],"default":null},
            {"name":"channel","type":["null","string"],"default":null},
            {"name":"classification","type":["null","string"],"default":null},
            {"name":"live_start_date_date","type":["null","string"],"default":null},
            {"name":"live_end_date_time","type":["null","string"],"default":null},
            {"name":"source","type":["null","string"],"default":null}
        ]
    }');


ADD JAR ${json_serde_to_use};

set mapred.output.compress=true;
set mapred.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;
set hive.exec.compress.output=true;
set avro.output.codec=snappy;
set hive.error.on.empty.partition=false;

USE ${database_to_use};

CREATE EXTERNAL TABLE IF NOT EXISTS amd_events_session_values (
    session_id            STRING
    , device_id           STRING
    , latitude            DOUBLE
    , longitude           DOUBLE
    , post_code           STRING
    , demo1               STRING
    , demo2               STRING
    , demo3               STRING
    , connection_type     STRING
    , streaming_type      STRING
    , ip_device_id        STRING
    , ingest_time         BIGINT
)
PARTITIONED BY (load_date STRING)
STORED AS PARQUET
LOCATION '${hdfs_table_location}';




USE ${database_to_use};


SET hive.execution.engine=spark;
SET hive.support.quoted.identifiers=none;
ADD JAR hdfs:///appdata/redfusion/dev/applications/wf_snowplow_events/jar/brickhouse-0.7.1-SNAPSHOT.jar;
CREATE TEMPORARY FUNCTION from_json AS 'brickhouse.udf.json.FromJsonUDF';
set mapreduce.map.memory.mb=8192;
set mapreduce.reduce.memory.mb=8192;

DROP TABLE IF EXISTS snowplow_events_raw_parq_tmp_${ingest_time} PURGE;

CREATE TABLE snowplow_events_raw_parq_tmp_${ingest_time}
STORED AS PARQUET
LOCATION '${table_location}'
AS

SELECT *
     , case when newcol.schema like "%unstruct_event%" then "unstruct_event"
     when newcol.schema like "%PerformanceTiming%" then "performance_timing"
     when newcol.schema like "%mobile_context%" then "mobile_context"
     when newcol.schema like "%web_page%" then "web_page"
     when newcol.schema like "%geolocation_context%" then "geolocation_context"
     else newcol.schema
     end as event_type
      ,newcol.schema as event_schema
      ,newcol.data as event_data

    FROM   (  SELECT  *
                , from_json(contexts, 'struct<schema:string,data:array<struct<schema:string,data:map<string,string>>>>') contexts_struct
              --  , split(split(INPUT__FILE__NAME,"/ingest_hour=")[1], "/")[1] as file_name
                 , INPUT__FILE__NAME as file_name
                 , row_number() over(PARTITION BY INPUT__FILE__NAME order by server_timestamp desc) as raw_row_num
        FROM
                ${tmp_table} ) a
lateral view explode(a.contexts_struct.data) explodetable as newcol;

---------------------------------------------------------------------

SnowPlow:

CREATE EXTERNAL TABLE IF NOT EXISTS snowplow_events_raw (

  app_id                      STRING
  , app_access                STRING
  , etl_tstamp                STRING
  , server_timestamp          STRING
  , device_timestamp          STRING
  , event                     STRING
  , event_guid                STRING
  , txn_id                    STRING
  , name_tracker              STRING
  , v_tracker                 STRING
  , v_collector               STRING
  , v_etl                     STRING
  , user_id                   STRING
  , device_ipaddress          STRING
  , user_fingerprint          STRING
  , domain_userid             STRING
  , domain_sessionidx         STRING
  , network_userid            STRING
  , geo_country               STRING
  , geo_region                STRING
  , geo_city                  STRING
  , geo_zipcode               STRING
  , geo_latitude              STRING
  , geo_longitude             STRING
  , geo_region_name           STRING
  , ip_isp                    STRING
  , ip_organization           STRING
  , ip_domain                 STRING
  , ip_netspeed               STRING
  , page_url                  STRING
  , page_name                 STRING
  , page_referrer             STRING
  , page_urlscheme            STRING
  , page_urlhost              STRING
  , page_urlport              STRING
  , page_path                 STRING
  , page_urlquery             STRING
  , page_urlfragment          STRING
  , refr_urlscheme            STRING
  , referral_url              STRING
  , referral_urlport          STRING
  , referral_urlpath          STRING
  , refr_urlquery             STRING
  , refr_urlfragment          STRING
  , referral_medium           STRING
  , refr_source               STRING
  , referral_term             STRING
  , mkt_medium                STRING
  , mkt_source                STRING
  , mkt_term                  STRING
  , mkt_content               STRING
  , mkt_campaign              STRING
  , contexts                  STRING
  , event_group               STRING
  , event_name                STRING
  , event_index               STRING
  , device_id                 STRING
  , se_value                  STRING
  , unstruct_event            STRING
  , tr_orderid                STRING
  , tr_affiliation            STRING
  , tr_total                  STRING
  , tr_tax                    STRING
  , tr_shipping               STRING
  , tr_city                   STRING
  , tr_state                  STRING
  , tr_country                STRING
  , ti_orderid                STRING
  , ti_sku                    STRING
  , ti_name                   STRING
  , ti_category               STRING
  , ti_price                  STRING
  , ti_quantity               STRING
  , pp_xoffset_min            STRING
  , pp_xoffset_max            STRING
  , pp_yoffset_min            STRING
  , pp_yoffset_max            STRING
  , useragent                 STRING
  , br_name                   STRING
  , device_browser            STRING
  , device_browser_version    STRING
  , br_type                   STRING
  , br_renderengine           STRING
  , device_browser_lang       STRING
  , br_features_pdf           STRING
  , br_features_flash         STRING
  , br_features_java          STRING
  , br_features_director      STRING
  , br_features_quicktime     STRING
  , br_features_realplayer    STRING
  , br_features_windowsmedia  STRING
  , br_features_gears         STRING
  , br_features_silverlight   STRING
  , br_cookies                STRING
  , br_colordepth             STRING
  , br_viewwidth              STRING
  , br_viewheight             STRING
  , os_name                   STRING
  , os_family                 STRING
  , os_manufacturer           STRING
  , os_timezone               STRING
  , dvce_type                 STRING
  , dvce_ismobile             STRING
  , dvce_screenwidth          STRING
  , dvce_screenheight         STRING
  , doc_charset               STRING
  , doc_width                 STRING
  , doc_height                STRING
  , tr_currency               STRING
  , tr_total_base             STRING
  , tr_tax_base               STRING
  , tr_shipping_base          STRING
  , ti_currency               STRING
  , ti_price_base             STRING
  , base_currency             STRING
  , geo_timezone              STRING
  , mkt_clickid               STRING
  , mkt_network               STRING
  , etl_tags                  STRING
  , dvce_sent_tstamp          STRING
  , refr_domain_userid        STRING
  , refr_dvce_tstamp          STRING
  , derived_contexts          STRING
  , domain_sessionid          STRING
  , derived_tstamp            STRING
  , event_vendor              STRING
  , event_name2               STRING
  , event_format              STRING
  , event_version             STRING
  , event_fingerprint         STRING
  , true_tstamp               STRING

)
PARTITIONED BY (load_date STRING, ingest_hour BIGINT)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE
LOCATION '${table_location}'
;

Map 
A complex data type representing an arbitrary set of key-value pairs. 
The key part is a scalar type, while the value part can be a scalar or another complex type (ARRAY, STRUCT, or MAP).
